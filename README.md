# VocabExtensionLLM
<br>
1. seed_data_test_split.py contains code to split seed dataset for train(90%) and test(10%)
2. tokenizer_specification.py -> code to find how two tokenizer are related, such as intersecting token, or avg tokenization length per sentence
3. combine_tokenizer.py -> contains code to combine two tokenizer (The one used for extended version)
